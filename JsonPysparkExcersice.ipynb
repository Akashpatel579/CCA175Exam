{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "            builder. \\\n",
    "            appName(\"CCA175ExamPreparation\"). \\\n",
    "            master(\"yarn\"). \\\n",
    "            config(\"spark.ui.port\", \"0\"). \\\n",
    "            getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gw01.itversity.com:46045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CCA175ExamPreparation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7278f70cf8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the solar hot water heater data which is stored as Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/user/akashpatel/exam_preparation/dataset/json/solar_hot_water_heater_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the schema of the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- APPLICATION_DATE: string (nullable = true)\n",
      " |-- COMPLETED_DATE: string (nullable = true)\n",
      " |-- DESCRIPTION: string (nullable = true)\n",
      " |-- ISSUED_DATE: string (nullable = true)\n",
      " |-- PERMIT_NUM: string (nullable = true)\n",
      " |-- PERMIT_TYPE: string (nullable = true)\n",
      " |-- POSTAL: string (nullable = true)\n",
      " |-- REVISION_NUM: long (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      " |-- STREET_DIRECTION: string (nullable = true)\n",
      " |-- STREET_NAME: string (nullable = true)\n",
      " |-- STREET_NUM: string (nullable = true)\n",
      " |-- STREET_TYPE: string (nullable = true)\n",
      " |-- STRUCTURE_TYPE: string (nullable = true)\n",
      " |-- WORK: string (nullable = true)\n",
      " |-- _id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view the first 5 records of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+------------------------------------------------------------------------------------------------------------------------+-----------+----------+--------------------------+------+------------+-------------+----------------+-----------+----------+-----------+-------------------+------------------------------+----+\n",
      "|APPLICATION_DATE|COMPLETED_DATE|                                                                                                             DESCRIPTION|ISSUED_DATE|PERMIT_NUM|               PERMIT_TYPE|POSTAL|REVISION_NUM|       STATUS|STREET_DIRECTION|STREET_NAME|STREET_NUM|STREET_TYPE|     STRUCTURE_TYPE|                          WORK| _id|\n",
      "+----------------+--------------+------------------------------------------------------------------------------------------------------------------------+-----------+----------+--------------------------+------+------------+-------------+----------------+-----------+----------+-----------+-------------------+------------------------------+----+\n",
      "|      2009-05-07|    2010-10-05|installation of solar hot water heating (supplemantary) system complete with dual panel (5m2) solar collector on roof...| 2009-05-11| 09 135007|Small Residential Projects|   M4K|           0|       Closed|                |  GRANDVIEW|         4|        AVE|SFD - Semi-Detached|Solar Domestic Hot Water (Res)|6561|\n",
      "|      2009-05-07|          null|                  Installation of a roof top solar water heating system with connection to potable water heating system | 2009-05-11| 09 135023|Small Residential Projects|   M4M|           0|   Inspection|                |      JONES|        81|        AVE|SFD - Semi-Detached|Solar Domestic Hot Water (Res)|6562|\n",
      "|      2009-05-11|    2011-01-14|                         Proposal to install a dual collector solar panel on the roof of existing SFD.  South Elevation.| 2009-05-20| 09 136204|Small Residential Projects|   M4J|           0|       Closed|                |   BOULTBEE|       114|        AVE|SFD - Semi-Detached|Solar Domestic Hot Water (Res)|6563|\n",
      "|      2009-06-11|    2010-03-03|Permit for installation of 2 solar collectors (certified model number 09-148270-CE) on roof of existing dwelling for ...| 2009-08-17| 09 148320|Small Residential Projects|   M4M|           0|       Closed|               E|    GERRARD|       899|         ST|    Converted House|Solar Domestic Hot Water (Res)|6564|\n",
      "|      2009-06-11|          null|Permit for installation of 2 solar collectors for solar hot water heating system. (09 148268 CE)Refer to related plum...| 2009-08-17| 09 148341|Small Residential Projects|   M4M|           0|Permit Issued|                |      LOGAN|       201|        AVE|SFD - Semi-Detached|Solar Domestic Hot Water (Res)|6565|\n",
      "+----------------+--------------+------------------------------------------------------------------------------------------------------------------------+-----------+----------+--------------------------+------+------------+-------------+----------------+-----------+----------+-----------+-------------------+------------------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print names of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['APPLICATION_DATE',\n",
       " 'COMPLETED_DATE',\n",
       " 'DESCRIPTION',\n",
       " 'ISSUED_DATE',\n",
       " 'PERMIT_NUM',\n",
       " 'PERMIT_TYPE',\n",
       " 'POSTAL',\n",
       " 'REVISION_NUM',\n",
       " 'STATUS',\n",
       " 'STREET_DIRECTION',\n",
       " 'STREET_NAME',\n",
       " 'STREET_NUM',\n",
       " 'STREET_TYPE',\n",
       " 'STRUCTURE_TYPE',\n",
       " 'WORK',\n",
       " '_id']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Description column from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+-----------+----------+--------------------------+------+------------+-------------+----------------+-----------+----------+-----------+-------------------+------------------------------+----+\n",
      "|APPLICATION_DATE|COMPLETED_DATE|ISSUED_DATE|PERMIT_NUM|               PERMIT_TYPE|POSTAL|REVISION_NUM|       STATUS|STREET_DIRECTION|STREET_NAME|STREET_NUM|STREET_TYPE|     STRUCTURE_TYPE|                          WORK| _id|\n",
      "+----------------+--------------+-----------+----------+--------------------------+------+------------+-------------+----------------+-----------+----------+-----------+-------------------+------------------------------+----+\n",
      "|      2009-05-07|    2010-10-05| 2009-05-11| 09 135007|Small Residential Projects|   M4K|           0|       Closed|                |  GRANDVIEW|         4|        AVE|SFD - Semi-Detached|Solar Domestic Hot Water (Res)|6561|\n",
      "|      2009-05-07|          null| 2009-05-11| 09 135023|Small Residential Projects|   M4M|           0|   Inspection|                |      JONES|        81|        AVE|SFD - Semi-Detached|Solar Domestic Hot Water (Res)|6562|\n",
      "|      2009-05-11|    2011-01-14| 2009-05-20| 09 136204|Small Residential Projects|   M4J|           0|       Closed|                |   BOULTBEE|       114|        AVE|SFD - Semi-Detached|Solar Domestic Hot Water (Res)|6563|\n",
      "|      2009-06-11|    2010-03-03| 2009-08-17| 09 148320|Small Residential Projects|   M4M|           0|       Closed|               E|    GERRARD|       899|         ST|    Converted House|Solar Domestic Hot Water (Res)|6564|\n",
      "|      2009-06-11|          null| 2009-08-17| 09 148341|Small Residential Projects|   M4M|           0|Permit Issued|                |      LOGAN|       201|        AVE|SFD - Semi-Detached|Solar Domestic Hot Water (Res)|6565|\n",
      "+----------------+--------------+-----------+----------+--------------------------+------+------------+-------------+----------------+-----------+----------+-----------+-------------------+------------------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(\"DESCRIPTION\")\n",
    "df.show(5, truncate = 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count unique value to each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+-----------+----------+-----------+------+------------+------+----------------+-----------+----------+-----------+--------------+----+---+\n",
      "|APPLICATION_DATE|COMPLETED_DATE|ISSUED_DATE|PERMIT_NUM|PERMIT_TYPE|POSTAL|REVISION_NUM|STATUS|STREET_DIRECTION|STREET_NAME|STREET_NUM|STREET_TYPE|STRUCTURE_TYPE|WORK|_id|\n",
      "+----------------+--------------+-----------+----------+-----------+------+------------+------+----------------+-----------+----------+-----------+--------------+----+---+\n",
      "|              72|            85|         69|       164|          1|    43|           1|     5|               4|        123|       127|          8|             4|   1|164|\n",
      "+----------------+--------------+-----------+----------+-----------+------+------------+------+----------------+-----------+----------+-----------+--------------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(*(countDistinct(col(c)).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many types of STATUS values are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,count, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------+\n",
      "|          STATUS|numbers_of_applications|\n",
      "+----------------+-----------------------+\n",
      "|   Permit Issued|                     21|\n",
      "|      Inspection|                     18|\n",
      "|       Cancelled|                     12|\n",
      "|Work Not Started|                      2|\n",
      "|          Closed|                    111|\n",
      "+----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df. \\\n",
    "    groupBy(col(\"STATUS\")). \\\n",
    "    agg(count(lit(1)).alias(\"numbers_of_applications\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.APPLICATION_DATE.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.ISSUED_DATE.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.COMPLETED_DATE.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the applications which are processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_issued = df. \\\n",
    "    filter(col(\"ISSUED_DATE\") != 'null')\n",
    "\n",
    "df_issued.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many days it took to process application?\n",
    "### Sort results in desending order by the days has been taken to process the application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-----------+---------------+\n",
      "| _id|APPLICATION_DATE|ISSUED_DATE|processing_days|\n",
      "+----+----------------+-----------+---------------+\n",
      "|6659|      2010-09-29| 2015-03-19|           1632|\n",
      "|6619|      2009-12-01| 2014-04-07|           1588|\n",
      "|6648|      2010-02-18| 2013-07-31|           1259|\n",
      "|6615|      2009-11-26| 2010-07-30|            246|\n",
      "|6567|      2009-06-19| 2010-02-03|            229|\n",
      "|6620|      2009-12-01| 2010-04-26|            146|\n",
      "|6621|      2009-12-01| 2010-04-26|            146|\n",
      "|6622|      2009-12-01| 2010-04-26|            146|\n",
      "|6712|      2011-12-15| 2012-04-03|            110|\n",
      "|6568|      2009-08-04| 2009-11-18|            106|\n",
      "|6714|      2012-06-19| 2012-09-26|             99|\n",
      "|6575|      2009-08-31| 2009-12-03|             94|\n",
      "|6565|      2009-06-11| 2009-08-17|             67|\n",
      "|6566|      2009-06-11| 2009-08-17|             67|\n",
      "|6564|      2009-06-11| 2009-08-17|             67|\n",
      "|6710|      2011-09-27| 2011-11-02|             36|\n",
      "|6713|      2012-04-05| 2012-05-11|             36|\n",
      "|6573|      2009-08-20| 2009-09-21|             32|\n",
      "|6715|      2012-10-15| 2012-11-16|             32|\n",
      "|6720|      2015-06-12| 2015-07-14|             32|\n",
      "+----+----------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_issued. \\\n",
    "    withColumn(\"processing_days\", datediff(col(\"ISSUED_DATE\"), col(\"APPLICATION_DATE\"))). \\\n",
    "    select(\"_id\", \"APPLICATION_DATE\", \"ISSUED_DATE\", \"processing_days\"). \\\n",
    "    orderBy(col(\"processing_days\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Water Heaters issued in year 2015?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_date in module pyspark.sql.functions:\n",
      "\n",
      "to_date(col, format=None)\n",
      "    Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n",
      "    :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n",
      "    using the optionally specified format. Specify formats according to\n",
      "    `SimpleDateFormats <http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html>`_.\n",
      "    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "    is omitted (equivalent to ``col.cast(\"date\")``).\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    .. versionadded:: 2.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, year\n",
    "\n",
    "help(to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_issued. \\\n",
    "    withColumn(\"ISSUED_YEAR\", year(to_date(col(\"ISSUED_DATE\"), format = \"yyyy-MM-dd\"))). \\\n",
    "    where(\"ISSUED_YEAR = 2015 \"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Water Heaters issued by year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+\n",
      "|ISSUED_YEAR|no of issued applications|\n",
      "+-----------+-------------------------+\n",
      "|       2017|                        1|\n",
      "|       2016|                        2|\n",
      "|       2015|                        3|\n",
      "|       2014|                        3|\n",
      "|       2013|                        2|\n",
      "|       2012|                        4|\n",
      "|       2011|                       10|\n",
      "|       2010|                       59|\n",
      "|       2009|                       78|\n",
      "+-----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_issued. \\\n",
    "    withColumn(\"ISSUED_YEAR\", year(to_date(col(\"ISSUED_DATE\"), format = \"yyyy-MM-dd\"))). \\\n",
    "    groupBy(col(\"ISSUED_YEAR\")). \\\n",
    "    agg(count(lit(1)).alias(\"no of issued applications\")). \\\n",
    "    orderBy(col(\"ISSUED_YEAR\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join operaiton :\n",
    "- Create clone dataframe of the actual data,\n",
    "- Filter the applications, for which application processiong at least started\n",
    "- generate column application processing_days column by APPLICATION_DATE, ISSUED_DATE and stored it into df_procession_days dataframe\n",
    "- JOIN the df_procession_days dataframe with the original dataframe - Use inner join. And make sure the processing_days columns is available with the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issued = df. \\\n",
    "    filter(col(\"ISSUED_DATE\") != 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procession_days = df_issued. \\\n",
    "    withColumn(\"processing_days\", datediff(col(\"ISSUED_DATE\"), col(\"APPLICATION_DATE\"))). \\\n",
    "    select(\"_id\", \"APPLICATION_DATE\", \"ISSUED_DATE\", \"processing_days\"). \\\n",
    "    orderBy(col(\"processing_days\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_procession_days.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+-----------+----------+--------------------+------+------------+-------------+----------------+-----------+----------+-----------+-------------------+--------------------+----+---------------+\n",
      "|APPLICATION_DATE|COMPLETED_DATE|ISSUED_DATE|PERMIT_NUM|         PERMIT_TYPE|POSTAL|REVISION_NUM|       STATUS|STREET_DIRECTION|STREET_NAME|STREET_NUM|STREET_TYPE|     STRUCTURE_TYPE|                WORK| _id|processing_days|\n",
      "+----------------+--------------+-----------+----------+--------------------+------+------------+-------------+----------------+-----------+----------+-----------+-------------------+--------------------+----+---------------+\n",
      "|      2009-05-07|    2010-10-05| 2009-05-11| 09 135007|Small Residential...|   M4K|           0|       Closed|                |  GRANDVIEW|         4|        AVE|SFD - Semi-Detached|Solar Domestic Ho...|6561|              4|\n",
      "|      2009-05-07|          null| 2009-05-11| 09 135023|Small Residential...|   M4M|           0|   Inspection|                |      JONES|        81|        AVE|SFD - Semi-Detached|Solar Domestic Ho...|6562|              4|\n",
      "|      2009-05-11|    2011-01-14| 2009-05-20| 09 136204|Small Residential...|   M4J|           0|       Closed|                |   BOULTBEE|       114|        AVE|SFD - Semi-Detached|Solar Domestic Ho...|6563|              9|\n",
      "|      2009-06-11|    2010-03-03| 2009-08-17| 09 148320|Small Residential...|   M4M|           0|       Closed|               E|    GERRARD|       899|         ST|    Converted House|Solar Domestic Ho...|6564|             67|\n",
      "|      2009-06-11|          null| 2009-08-17| 09 148341|Small Residential...|   M4M|           0|Permit Issued|                |      LOGAN|       201|        AVE|SFD - Semi-Detached|Solar Domestic Ho...|6565|             67|\n",
      "+----------------+--------------+-----------+----------+--------------------+------+------------+-------------+----------------+-----------+----------+-----------+-------------------+--------------------+----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df. \\\n",
    "    join(df_procession_days, df._id == df_procession_days._id). \\\n",
    "    select(df[\"*\"], df_procession_days[\"processing_days\"]). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df. \\\n",
    "    join(df_procession_days, df._id == df_procession_days._id). \\\n",
    "    select(df[\"*\"], df_procession_days[\"processing_days\"]). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out the yearwise First 3 Applicants of the Solar Water Heater plant? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col, to_date, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Window. \\\n",
    "    partitionBy(col(\"APPLICATION_YEAR\")). \\\n",
    "    orderBy(col(\"APPLICATION_YEAR\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+------+\n",
      "| _id|APPLICATION_YEAR|NUMBER|\n",
      "+----+----------------+------+\n",
      "|6724|            2017|     1|\n",
      "|6721|            2016|     1|\n",
      "|6722|            2016|     2|\n",
      "|6723|            2016|     3|\n",
      "|6719|            2015|     1|\n",
      "|6720|            2015|     2|\n",
      "|6717|            2014|     1|\n",
      "|6718|            2014|     2|\n",
      "|6716|            2013|     1|\n",
      "|6713|            2012|     1|\n",
      "|6714|            2012|     2|\n",
      "|6715|            2012|     3|\n",
      "|6702|            2011|     1|\n",
      "|6703|            2011|     2|\n",
      "|6704|            2011|     3|\n",
      "|6645|            2010|     1|\n",
      "|6646|            2010|     2|\n",
      "|6647|            2010|     3|\n",
      "|6561|            2009|     1|\n",
      "|6562|            2009|     2|\n",
      "|6563|            2009|     3|\n",
      "+----+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df. \\\n",
    "    withColumn(\"APPLICATION_YEAR\", year(to_date(col(\"APPLICATION_DATE\"), format= \"yyyy-MM-dd\"))). \\\n",
    "    withColumn(\"NUMBER\", row_number().over(spec)). \\\n",
    "    filter(\"NUMBER <= 3\"). \\\n",
    "    select(col(\"_id\"), col(\"APPLICATION_YEAR\"), col(\"NUMBER\")). \\\n",
    "    orderBy(col(\"APPLICATION_YEAR\").desc(), col(\"NUMBER\")). \\\n",
    "    show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Unique Application's Dates and view in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|APPLICATION_DATE|\n",
      "+----------------+\n",
      "|      2009-05-07|\n",
      "|      2009-05-11|\n",
      "|      2009-06-11|\n",
      "|      2009-06-19|\n",
      "|      2009-08-04|\n",
      "|      2009-08-08|\n",
      "|      2009-08-20|\n",
      "|      2009-08-31|\n",
      "|      2009-09-09|\n",
      "|      2009-10-02|\n",
      "|      2009-10-23|\n",
      "|      2009-10-27|\n",
      "|      2009-11-03|\n",
      "|      2009-11-04|\n",
      "|      2009-11-09|\n",
      "|      2009-11-10|\n",
      "|      2009-11-13|\n",
      "|      2009-11-17|\n",
      "|      2009-11-20|\n",
      "|      2009-11-23|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"APPLICATION_DATE\"]). \\\n",
    "    distinct(). \\\n",
    "    orderBy(col(\"APPLICATION_DATE\").asc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data partition by year, with gzip compression and parquet format\n",
    "\n",
    "#### since we have more where query on YEAR column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df. \\\n",
    "    withColumn(\"YEAR\", year(to_date(col(\"APPLICATION_DATE\"), format = \"yyyy-MM-dd\"))). \\\n",
    "    write.parquet(\"/user/akashpatel/exam_preparation/solar_water_heater/output\", mode= \"overwrite\", partitionBy= \"YEAR\", compression= \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 items\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 21:22 /user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2009\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 21:22 /user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2010\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 21:22 /user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2011\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 21:22 /user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2012\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 21:22 /user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2013\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 21:22 /user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2014\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 21:22 /user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2015\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 21:22 /user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2016\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 21:22 /user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2017\n",
      "-rw-r--r--   2 akashpatel hdfs          0 2020-04-30 21:22 /user/akashpatel/exam_preparation/solar_water_heater/output/_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "\n",
    "hdfs dfs -ls /user/akashpatel/exam_preparation/solar_water_heater/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read above the partitioned data, which is in parquet format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 akashpatel hdfs       6520 2020-04-30 21:22 /user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2009/part-00000-200ddc0b-ba61-4f0f-9b13-661dc429a5db.c000.gz.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "\n",
    "hdfs dfs -ls /user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df = spark.read.parquet(\"/user/akashpatel/exam_preparation/solar_water_heater/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- APPLICATION_DATE: string (nullable = true)\n",
      " |-- COMPLETED_DATE: string (nullable = true)\n",
      " |-- ISSUED_DATE: string (nullable = true)\n",
      " |-- PERMIT_NUM: string (nullable = true)\n",
      " |-- PERMIT_TYPE: string (nullable = true)\n",
      " |-- POSTAL: string (nullable = true)\n",
      " |-- REVISION_NUM: long (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      " |-- STREET_DIRECTION: string (nullable = true)\n",
      " |-- STREET_NAME: string (nullable = true)\n",
      " |-- STREET_NUM: string (nullable = true)\n",
      " |-- STREET_TYPE: string (nullable = true)\n",
      " |-- STRUCTURE_TYPE: string (nullable = true)\n",
      " |-- WORK: string (nullable = true)\n",
      " |-- _id: long (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read specific data of year - 2009, from the partitioned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df_2009_year_specific = spark.read.parquet(\"/user/akashpatel/exam_preparation/solar_water_heater/output/YEAR=2009\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- APPLICATION_DATE: string (nullable = true)\n",
      " |-- COMPLETED_DATE: string (nullable = true)\n",
      " |-- ISSUED_DATE: string (nullable = true)\n",
      " |-- PERMIT_NUM: string (nullable = true)\n",
      " |-- PERMIT_TYPE: string (nullable = true)\n",
      " |-- POSTAL: string (nullable = true)\n",
      " |-- REVISION_NUM: long (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      " |-- STREET_DIRECTION: string (nullable = true)\n",
      " |-- STREET_NAME: string (nullable = true)\n",
      " |-- STREET_NUM: string (nullable = true)\n",
      " |-- STREET_TYPE: string (nullable = true)\n",
      " |-- STRUCTURE_TYPE: string (nullable = true)\n",
      " |-- WORK: string (nullable = true)\n",
      " |-- _id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_df_year_specific.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data partition by year, with lzo compression and orc format\n",
    "\n",
    "#### since we have more where query on YEAR column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"YEAR\", year(to_date(\"APPLICATION_DATE\"))). \\\n",
    "    write. \\\n",
    "    orc(\"/user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc\", mode= \"overwrite\", partitionBy= \"YEAR\", compression= \"lzo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 items\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 22:01 /user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc/YEAR=2009\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 22:01 /user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc/YEAR=2010\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 22:01 /user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc/YEAR=2011\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 22:01 /user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc/YEAR=2012\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 22:01 /user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc/YEAR=2013\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 22:01 /user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc/YEAR=2014\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 22:01 /user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc/YEAR=2015\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 22:01 /user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc/YEAR=2016\n",
      "drwxr-xr-x   - akashpatel hdfs          0 2020-04-30 22:01 /user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc/YEAR=2017\n",
      "-rw-r--r--   2 akashpatel hdfs          0 2020-04-30 22:01 /user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc/_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "\n",
    "hdfs dfs -ls /user/akashpatel/exam_preparation/solar_water_heater/partition_output_orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/akashpatel/exam_preparation/solar_water_heater/bucket_partition_output\n"
     ]
    }
   ],
   "source": [
    "# If you want to rermove directory from the hdfs \n",
    "%%sh\n",
    "\n",
    "hdfs dfs -rm -R -skipTrash /user/akashpatel/exam_preparation/solar_water_heater/bucket_partition_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE ::  BucketBy only works with saveAsTable, save as files doesn't work in Spark\n",
    "\n",
    "### Save data partition by year, as spark metastore - table\n",
    "\n",
    "#### also create Bucket by 5, Status column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"YEAR\", year(to_date(\"APPLICATION_DATE\"))). \\\n",
    "    write. \\\n",
    "    bucketBy(5, \"STATUS\"). \\\n",
    "    saveAsTable('bucketed_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tbl = spark.read.table(\"bucketed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- APPLICATION_DATE: string (nullable = true)\n",
      " |-- COMPLETED_DATE: string (nullable = true)\n",
      " |-- ISSUED_DATE: string (nullable = true)\n",
      " |-- PERMIT_NUM: string (nullable = true)\n",
      " |-- PERMIT_TYPE: string (nullable = true)\n",
      " |-- POSTAL: string (nullable = true)\n",
      " |-- REVISION_NUM: long (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      " |-- STREET_DIRECTION: string (nullable = true)\n",
      " |-- STREET_NAME: string (nullable = true)\n",
      " |-- STREET_NUM: string (nullable = true)\n",
      " |-- STREET_TYPE: string (nullable = true)\n",
      " |-- STRUCTURE_TYPE: string (nullable = true)\n",
      " |-- WORK: string (nullable = true)\n",
      " |-- _id: long (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tbl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+\n",
      "|              t|  x|\n",
      "+---------------+---+\n",
      "|Jan-17 00:00:00|  a|\n",
      "|Apr-19 00:00:00|  b|\n",
      "+---------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Jan-17 00:00:00\",'a'),(\"Apr-19 00:00:00\",'b')], ['t','x'])\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                 dt|\n",
      "+-------------------+\n",
      "|2017-01-01 00:00:00|\n",
      "|2018-12-30 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as psf\n",
    "df.select(psf.to_timestamp(psf.col('t'), 'MMM-YY HH:MM:ss').alias('dt')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
